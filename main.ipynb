{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFourEnvironment:\n",
    "    def __init__(self, rows=6, cols=7):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
    "        self.current_player = 1\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        return np.array([0 if self.board[0, col] == 0 else 1 for col in range(self.cols)])\n",
    "\n",
    "    def is_valid_action(self, action):\n",
    "        if action < 0 or action >= self.cols:\n",
    "            return False\n",
    "        return self.get_valid_actions()[action] == 0\n",
    "\n",
    "    def make_move(self, action):\n",
    "        for row in range(self.rows - 1, -1, -1):\n",
    "            if self.board[row, action] == 0:\n",
    "                self.board[row, action] = self.current_player\n",
    "                break\n",
    "        self.current_player = 3 - self.current_player  # Switch player\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.board.copy()\n",
    "\n",
    "    def check_line(self, line):\n",
    "        for player in [1, 2]:\n",
    "            for i in range(len(line) - 3):\n",
    "                if np.all(line[i:i + 4] == player):\n",
    "                    return player\n",
    "        return 0\n",
    "\n",
    "    def get_winner(self):\n",
    "        # Check for a winner in rows, columns, and diagonals\n",
    "        for i in range(self.rows):\n",
    "            row_result = self.check_line(self.board[i, :])\n",
    "            if row_result:\n",
    "                return row_result\n",
    "\n",
    "        for j in range(self.cols):\n",
    "            col_result = self.check_line(self.board[:, j])\n",
    "            if col_result:\n",
    "                return col_result\n",
    "\n",
    "        for i in range(self.rows - 3):\n",
    "            for j in range(self.cols - 3):\n",
    "                diag_result = self.check_line(self.board[i:i + 4, j:j + 4].diagonal())\n",
    "                if diag_result:\n",
    "                    return diag_result\n",
    "\n",
    "                rev_diag_result = self.check_line(np.fliplr(self.board[i:i + 4, j:j + 4]).diagonal())\n",
    "                if rev_diag_result:\n",
    "                    return rev_diag_result\n",
    "\n",
    "        return 0  # No winner yet\n",
    "\n",
    "    def step(self, action):\n",
    "        if not self.is_valid_action(action):\n",
    "            print(\"Invalid action:\", action)\n",
    "            raise ValueError(\"Invalid action. Please choose a valid action.\")\n",
    "\n",
    "        self.make_move(action)\n",
    "        state = self.get_state()\n",
    "        done = self.get_winner() != 0 or not any(self.board[0, :] == 0)  # Check for a winner or a full board\n",
    "        return state, self.current_player, done\n",
    "    \n",
    "    def display_board(self):\n",
    "        for row in self.board:\n",
    "            print(\"|\", end=\" \")\n",
    "            for cell in row:\n",
    "                print(cell, end=\" \")\n",
    "            print(\"|\")\n",
    "        print(\"+-----------------+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(6 * 7, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 6 * 7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.99, epsilon=0.1):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Define the Q-networks\n",
    "        self.policy_net = DQN().to(self.device)\n",
    "        self.target_net = DQN().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Define the optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "        # Experience Replay Memory\n",
    "        self.memory = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        print(valid_actions)\n",
    "        if random.random() < self.epsilon:\n",
    "            print('random')\n",
    "            idx = np.nonzero(valid_actions == 0)[0]\n",
    "            return torch.tensor(random.choice(idx), dtype=torch.long).to(self.device)\n",
    "        else:\n",
    "            print('not random')\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(self.device)\n",
    "                q_values = self.policy_net(state).cpu().numpy()\n",
    "                q_values[0][valid_actions == 1] = float('-inf')\n",
    "\n",
    "                print(q_values)\n",
    "                return torch.tensor(np.argmax(q_values), dtype=torch.long).to(self.device)\n",
    "\n",
    "\n",
    "    def store_transition(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def train(self, batch_size=32):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch.action).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(batch.next_state).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).to(self.device)\n",
    "\n",
    "        q_values = self.policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        next_q_values = self.target_net(next_state_batch).max(1)[0].detach()\n",
    "        # expected_q_values = reward_batch + self.gamma * next_q_values\n",
    "        expected_q_values = reward_batch + 0.99 * next_q_values\n",
    "\n",
    "        loss = F.smooth_l1_loss(q_values, expected_q_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ConnectFourEnvironment()\n",
    "input_size = 6 * 7  # Adjust according to your state size\n",
    "output_size = 7  # Assuming your output is the number of valid actions\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = DQNAgent(input_size, output_size, device)\n",
    "\n",
    "num_episodes = 1000\n",
    "target_update_frequency = 10\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.get_state()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        print(\"Action:\", action)\n",
    "        env.display_board()\n",
    "        next_state, reward, done = env.step(action)\n",
    "        print(\"Done:\", done)\n",
    "        agent.store_transition(Transition(state, action, next_state, reward))\n",
    "        agent.train()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if episode % target_update_frequency == 0:\n",
    "        agent.update_target_network()\n",
    "        \n",
    "    env.reset()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
